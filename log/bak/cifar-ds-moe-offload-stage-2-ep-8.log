[2022-09-10 21:01:59,590] [WARNING] [runner.py:178:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2022-09-10 21:01:59,725] [INFO] [runner.py:504:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 cifar10_deepspeed.py --log-interval 100 --deepspeed --deepspeed_config ds_config_offload.json --moe --ep-world-size 2 --num-experts 8 --top-k 1 --noisy-gate-policy RSample --moe-param-group
[2022-09-10 21:02:01,271] [INFO] [launch.py:136:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2022-09-10 21:02:01,271] [INFO] [launch.py:143:main] nnodes=1, num_local_procs=4, node_rank=0
[2022-09-10 21:02:01,271] [INFO] [launch.py:155:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2022-09-10 21:02:01,271] [INFO] [launch.py:156:main] dist_world_size=4
[2022-09-10 21:02:01,271] [INFO] [launch.py:158:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2022-09-10 21:02:03,040] [INFO] [comm.py:635:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W ProcessGroupNCCL.cpp:1569] Rank 2 using best-guess GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 3 using best-guess GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
Files already downloaded and verified
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
 deer  ship truck truck
[2022-09-10 21:02:11,245] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 4 | expert_parallel_size: 2
-----------------------------------------before create moe param_groups parameters = {}
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

-----------------------------------------after  create moe param_groups parameters = {}
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

-----------------------------------------after split_params_into_different_moe_groups_for_optimizer(parameters)
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

[2022-09-10 21:02:11,249] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.3+a691ec60, git-hash=a691ec60, git-branch=master
[2022-09-10 21:02:11,251] [WARNING] [config_utils.py:64:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
No existing process group found, creating a new group named: ep_size_2
[2022-09-10 21:02:11,256] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 2
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
truck   dog   car  frog
 frog   cat plane  deer
-----------------------------------------before create moe param_groups parameters = {}
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

-----------------------------------------after  create moe param_groups parameters = {}
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

-----------------------------------------after split_params_into_different_moe_groups_for_optimizer(parameters)
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

plane  frog  ship  bird
[2022-09-10 21:02:12,715] [WARNING] [config_utils.py:64:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
-----------------------------------------before create moe param_groups parameters = {}
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

-----------------------------------------after  create moe param_groups parameters = {}
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

-----------------------------------------after split_params_into_different_moe_groups_for_optimizer(parameters)
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

[2022-09-10 21:02:12,719] [WARNING] [config_utils.py:64:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
No existing process group found, creating a new group named: ep_size_2
No existing process group found, creating a new group named: ep_size_2
-----------------------------------------before create moe param_groups parameters = {}
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

-----------------------------------------after  create moe param_groups parameters = {}
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

-----------------------------------------after split_params_into_different_moe_groups_for_optimizer(parameters)
 Nvidia-smi: 1.1319580078125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

[2022-09-10 21:02:12,726] [WARNING] [config_utils.py:64:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
No existing process group found, creating a new group named: ep_size_2
[2022-09-10 21:02:12,733] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_2 with ranks: [0, 2]
[2022-09-10 21:02:12,744] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_2 with ranks: [1, 3]
[2022-09-10 21:02:12,755] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_2 with ranks: [0, 1]
[2022-09-10 21:02:12,755] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_2 with ranks: [2, 3]
[2022-09-10 21:02:13,115] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.001000, betas=(0.800000, 0.999000), weight_decay=0.000000, adam_w=1
[2022-09-10 21:02:15,053] [INFO] [logging.py:68:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2022-09-10 21:02:15,054] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = {basic_optimizer.__class__.__name__}
[2022-09-10 21:02:15,054] [INFO] [utils.py:53:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2022-09-10 21:02:15,054] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
[2022-09-10 21:02:15,055] [INFO] [stage_1_and_2.py:134:__init__] Reduce bucket size 50000000
[2022-09-10 21:02:15,055] [INFO] [stage_1_and_2.py:135:__init__] Allgather bucket size 50000000
[2022-09-10 21:02:15,055] [INFO] [stage_1_and_2.py:136:__init__] CPU Offload: True
[2022-09-10 21:02:15,055] [INFO] [stage_1_and_2.py:137:__init__] Round robin gradient partitioning: False
Traceback (most recent call last):
  File "cifar10_deepspeed.py", line 258, in <module>
    args=args, model=net, model_parameters=parameters, training_data=trainset)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/__init__.py", line 134, in initialize
    config_params=config_params)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 320, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 1144, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 1424, in _configure_zero_optimizer
    elastic_checkpoint=self.zero_elastic_checkpoint())
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 198, in __init__
    self._configure_moe_settings()
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 535, in _configure_moe_settings
    assert any([self.is_moe_group(group) for group in self.optimizer.param_groups]), "The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer"
AssertionError: The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.001000, betas=(0.800000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.001000, betas=(0.800000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.001000, betas=(0.800000, 0.999000), weight_decay=0.000000, adam_w=1
Traceback (most recent call last):
  File "cifar10_deepspeed.py", line 258, in <module>
    args=args, model=net, model_parameters=parameters, training_data=trainset)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/__init__.py", line 134, in initialize
    config_params=config_params)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 320, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 1144, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 1424, in _configure_zero_optimizer
    elastic_checkpoint=self.zero_elastic_checkpoint())
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 198, in __init__
    self._configure_moe_settings()
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 535, in _configure_moe_settings
    assert any([self.is_moe_group(group) for group in self.optimizer.param_groups]), "The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer"
AssertionError: The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer
Traceback (most recent call last):
  File "cifar10_deepspeed.py", line 258, in <module>
    args=args, model=net, model_parameters=parameters, training_data=trainset)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/__init__.py", line 134, in initialize
    config_params=config_params)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 320, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 1144, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 1424, in _configure_zero_optimizer
Traceback (most recent call last):
  File "cifar10_deepspeed.py", line 258, in <module>
    elastic_checkpoint=self.zero_elastic_checkpoint())
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 198, in __init__
    args=args, model=net, model_parameters=parameters, training_data=trainset)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/__init__.py", line 134, in initialize
    self._configure_moe_settings()
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 535, in _configure_moe_settings
    config_params=config_params)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 320, in __init__
    assert any([self.is_moe_group(group) for group in self.optimizer.param_groups]), "The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer"
AssertionError: The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 1144, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/engine.py", line 1424, in _configure_zero_optimizer
    elastic_checkpoint=self.zero_elastic_checkpoint())
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 198, in __init__
    self._configure_moe_settings()
  File "/home/cc/.local/lib/python3.6/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 535, in _configure_moe_settings
    assert any([self.is_moe_group(group) for group in self.optimizer.param_groups]), "The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer"
AssertionError: The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer
[2022-09-10 21:02:16,345] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 38352
[2022-09-10 21:02:16,346] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 38353
[2022-09-10 21:02:16,393] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 38354
[2022-09-10 21:02:16,412] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 38355
[2022-09-10 21:02:16,427] [ERROR] [launch.py:292:sigkill_handler] ['/usr/bin/python3', '-u', 'cifar10_deepspeed.py', '--local_rank=3', '--log-interval', '100', '--deepspeed', '--deepspeed_config', 'ds_config_offload.json', '--moe', '--ep-world-size', '2', '--num-experts', '8', '--top-k', '1', '--noisy-gate-policy', 'RSample', '--moe-param-group'] exits with return code = 1
